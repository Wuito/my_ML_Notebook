>文中程序以Tensorflow-2.6.0为例
>部分概念包含笔者个人理解，如有遗漏或错误，欢迎评论或私信指正。
>截图部分引用自北京大学机器学习公开课

## 人工智能算法的主流分类
首先明白一个概念，广义上的人工智能算法并不是只有Machine Learning或Deep Learning，而是一个相对的，能够使用计算机模拟人类智能在一定场景下自动实现一些功能。所以系统控制论中的很多最优控制算法同样可以称之为智能算法。在后面的描述中我们会看到NN结构在学习参数的过程中其实很类似于一个通过反馈系统，能自动矫正控制器参数的控制系统。
首先，学界将算法分类为以下三种主要类型：
**行为主义**，构建感知控制系统（控制论，经典控制论或者现代控制理论中的各种矫正模型和最优控制算法也属于智能控制算法的一种）
**符号主义**，基于数学表达和运算求解（通过数学建模，将复杂问题转化为完整的数学系统，通过数学方法求出解，进而解决问题，例如专家系统）
**连接主义**，基于仿生学，构建类神经网络，神经网络模型则类似于黑盒性质，通过自动的学习和参数调整，可以实现对复杂非线性方程的描述。

传统的建模方法和控制理论已经发展的非常完善，但是依然在实际应用中存在局限性。基于网络模型的算法，通过学习可以快速的拟合各类复杂的非线性函数，并且在数学设计上相比众多建模方法原理简单明了，便于设计和使用。


-------------

## 机器学习与概率论

在回归拟合数据时，很具拟合对象，可以把分类问题视为一种简答的逻辑回归。在逻辑回归中算法不去拟合一段数据而是判断输入的数据是哪一个种类。有很多算法既可以实现线性回归也可以实现逻辑回归。
|         | 线性回归         | 逻辑回归  |
|:-------------:|:-------------:|:-----:|
| 目的     | 预测 |分类 |
|  $y^{(i)}$   | 未知     |   （0,1）|
| 函数 | 拟合函数     |   预测函数 |
| 参数计算方式| 最小二乘法      |    极大似然估计 |

如何实现概率上的分布？

在概率论中当拥有一组足够大样本数据时，那么这组数据的期望和方差会收敛于这个数据分布的期望和方差。
对基本的切比雪夫不等式， $$ E(I_{\left|X-\mu \right|}>\alpha )=P(\left|X-\mu \right|\ge\alpha)\le\frac{DX}{\alpha^2} $$
由此出发可以推导出切比雪夫大数定律、伯努利大数定律，中心极限定理等概率论的基石公式。
那么假如现在我们有一组样本数据，样布数据来自某个未知分布。是否可以找到一个含参函数，可以百分百拟合样本服从的分布？
$$ \exists f(X|\theta )?\Rightarrow  \lim_{\varepsilon \to 0^+} P(|f(X)-x|<\varepsilon )=1 $$
从这个问题出发，在统计学上我们已经认识了矩估计、极大似然估计两种方法来计算这个函数中的具体参数。
对计算机来说是否有其他方法？
- 多层判断：如果样本分布在有限空间内，总可以找到一个符合分布的树状判断结构，一层一层递推判断并构建新分支，最后得到完整的符合分布的判断结构。
- 迭代学习：通过循环输入样本参数，计算函数的输出是否符合要求，再根据差距大小，调整函数构成和参数值，最后得到函数结果。

树状判断很好理解，那迭代学习如何实现：
首先是需要知道函数计算得到的分布和实际的分布之间的差距。继续上面的公式我们可以再加入一个函数，用来计算当前函数结果是否准确
$$ loss （f(x|\theta )-F(X)）$$
我们把这样的函数称之为代价函数，在深度学习中也可称之为损失函数。当有样本和真确分布的答案时（有监督学习）可以直接计算函数输出到实际的距离。对于没有正确答案的回归时，此时变为求解函数到所有样本点之间的距离：
$$ L(x, \theta) = \frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)}) $$
当存在参数使得函数到所有样本距离最小的时候：
$$ \exists\theta\Rightarrow\min L(x,\theta)=\min\frac{1}{m}\sum_{i=1}^m(f(x^{(i)}|\theta)-y^{(i)}) $$
此时可以称之为找到了一个函数可以再概率上最大程度的拟合样本的分布情况。
机器学习中很多方法的目的就是，找到科学的方法，让计算机根据样本数据找到合适的函数 f 和合适的参数，并最终能够应用到新的场景对新样本做出预测或判断。
现在假设机器学习样本数据时直接使用上述的差值平均值作为代价，那如何求解参数来使差值最小？答案已经呼之欲出————梯度。$$ \frac{\partial L}{\partial \theta}=\dot{L} (x, \theta) = {\frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})}' $$ $$ {\frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})}'_\theta \Rightarrow {\frac{1}{m}\sum_{i=1}^m(f'_\theta (x^{(i)})-y^{(i)})} $$
计算梯度时，输入的样本是已知数据，需要变化的是函数的参数，通过计算代价函数对变量的梯度，就可以知道在输入样本的前提下，函数朝着什么方向变化参数能使输出的差值变小，此时计算机只需根据梯度更新参数。通过不断的循环这个步骤就达到了学习参数的目的。

通过上面的介绍，简单知道了学习的过程。实际上在机器学习中远没有这样简单，从函数结构，代价函数，到参数更新，输入输出等等，每一个环节都有着详细的内容和不同的方法来适应不同的数据场景。

## 认识神经网络
通常构建一个NN网络我们需要以下内容：
#### 1、什么是神经元模型
在中学生物中学习过生物神经元，包含树突，轴突，胞体等结构，大量的神经元细胞之间可以相互连接。同样可以在计算机中模拟出单一神经元的MP模型：

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/bbed94f370bf4afc996baddf5196f72a.png)
对于这样的单个神经元模型，当输入多个来源的数据，通过每个输入乘以各自权重系数后求和，在经由一些特定的输出函数后得到结果。可以写成向量函数：$$ \mathit{Y=F(X\times Y+b)}  $$其中Y是输出，X是输入，W是网络权重，b是偏置系数，f是输出函数
假设我们有很多个神经元，他们彼此连接，用多个上面的神经元模型并列就可以构成一层神经网络，再使用多层神经网络级联就可以更加复杂的网络结构。那么我们就可以一定的权重参数下计算输入对应的特地输出
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/42efc344516d455b83e689630b5b8467.png)
上面的例子中输入4个X，输出3个Y，特定的输入会对应特定的输出。这个输出的数据就可以进一步设计，通过判断来得到最后总共的结果。

#### 2、常见的神经网络构建流程：
 - 准备数据：准备大量的“特征\标签”数据 ，网络通过学习二者之间的差距，来修改权重参数	
 - 搭建网络结构，设计特定的网络层连接
 - 通过反向传播训练网络，优化参数
 - 保存模型，使用前向传播推理计算结果

通常的分类问题中，大多数都可以数学建模使用判别式的计算来实现目标结果。但是数学建模和判别的计算会随着问题的复杂而不断复杂化，所以上述流程的NN网络算法就显得简单明了，容易使用。
#### 3、模型的训练
在简单的MLP模型中，假设网络中每个权重都首次赋予某个初值，输入数据后进行一次计算，会得到第一次的输出，此时该输出与输入的标签之间会存在差距。利用损失函数来计算输出Y和正确标签之间的差距，通过反向传播（反馈修正）更改网络中的权重系数。在损失函数的计算中，***当损失函数的输出值loss最小时，网络中的权重和偏置系数达到了最优值***。
所以模型的训练过程可以理解为：输入一组数据，通过一次前向推理得到计算结果，损失函数计算标注数据和推理结果的差距，计算损失函数当前的梯度下降方向，按照梯度下降方向反过来修改网络中的权重参数，保存网络，再重复上述过程。直到最后网络输出的结果已经能够足够精准的预测输出停止。这个反复的过程就称之为网络的训练。在训练的过程中循环的次数称之为迭代次数Epoch ,一次循环中输入网络的数据量大小称之为batch_size。其中Epoch 影响训练时间的长短，batch_size过大则需要更大的显存来训练。
简化的流程图可以理解为：（这里忽略输入和输出环节的其它步骤）
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/2267d340f65c433382914b7da1314b79.png)
实际的应用中data在输入之前会经过一定的预处理和分组，输出也会由不同的设计来实现特定的目的。
#### 4、反向传播在做什么：
理解反向传播之前，我们需要清楚***前向传播***在做什么，通常在前向传播的过程中，我们直接给网络输入一组数据，让网络计算得到结果，就完成了一次前向传播。在训练的过程中依次完整的训练是包含一次前向传播+一次反向传播+参数修改完成的（这里暂时忽略一些重要的细节），训练完成后，使用权重模型来***推理预测***的过程，就是单次的前向传播。
所以权重成为了至关重要的数据，修改权重就是让网络学习数据输入到输出的映射关系，故而设计反向传播，通过输出的数据反过来修正权重参数。
在反向传播的过程中，人们设计了损失函数来表达计算结果和实际结果之间的误差。通过损失函数量化了输出误差的计算，使得计算机能够自动进行反向矫正。使得修改权重参数的问题转化为：如何能让输出的损失函数值变的更小。
回顾梯度的定义：函数对参数求***偏导数***，梯度减小的方向就是函数减小的方向，就是函数的最大方向向量。沿着函数梯度下降的方向就可以使损失函数输出变小。
所以可以利用***计算损失函数梯度下降的方向来寻找权重修改的方向***，来计算得到损失函数的最小值。
在梯度下降法中： 权重参数的计算：
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b2bd4b28ea414e8c8a6c9eec89b4d064.png)
式中，lr是学习率，是一个超参数。学习率较大时，W的更新幅度较快，学习率过小时W更新幅度较小，Floss就是损失函数。通过一次次的迭代，最终可以得到一个使输出Loss最小的W。最常用的损失函数有均方误差：MSE
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/532c222580724b31849387df83e5cfe2.png)
MSE直接计算Y和I之间每个数据点之间的差之和并最后求平均。

#### 5、反向传播——梯度下降
上面已经介绍了我们希望使用梯度下降来找到合适的权重参数W，来使得损失函数的输出能够最小。由于梯度减小的方向就是函数减小的方向，所以可以使用梯度下降法来计算最优参数。
在基础的神经元模型中包含权重参数w，偏置系数b，y的输出取决于w和b共同作用。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/53514c8b28bb4aa781fcf207e1112726.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/c10416e59ef94749bc7efc94cc32c8dc.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/08da51fefa114696823ca1e95244d763.png)
在梯度下降的公式中学习率lr显得十分重要，当学习率设置的太小时，收敛将变得十分缓慢，等学习率变得过大时梯度的值可能在最小值之间来回震荡，导致无法收敛。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b546e5a6162947d0b33be3ad34d9a4a4.png)
在反向传播的过程中，算法会***从前向后，一层一层的用损失函数计算每层参数的偏导数，并根据结果更新这层的所有参数。***
#### 6、张量的概念
我们知道神经网络的计算中每个输入是由矩阵形式的数据构成的，为了便于网络计算以及使用统一框架，我们需要使用tensorflow中提供的张量模型。张量其实质上就是一个多维数组。数组的维数就是张量的阶数，一个0阶的数称为标量（scalar），1阶数组称为向量，二阶数组称为矩阵，三阶及以上的数组称为张量。
张量中的数量型数据类型常见的有三种： `tf.int32` ，`tf.float32` ， `tf.float64`
此外还有数以外的数据类型，如 布尔型 `tf.bool` ,  字符串型 `tf.string`
```
# （执行代码前请先关注下文的程序版本）创建一个张量：
# tf.constant(内容，dtype=数据类型)
num = tf.constant([1,2], dtype=tf.int64)	# 数
usr_bool = tf.constant([True])	·				# 布尔 or: tf.constant([False])	
usr_string = tf.constant("Hello world!")	# 字符串
```
通常来说我们会从numpy中导入数据到tensor：
```
a = np.arange(0, 5)
b = tf.convert_to_tensor(a, dtype=tf.int64)
print(a)
print(b)
```
得到输出：
[0 1 2 3 4]
tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)
其中a为一行5列，b为1行5列，shape表示维度，dtype表示数据类型

当然我们也可以从头创建Tensor:
```
a1 = tf.zeros(3)		# 全零型,一阶
a2 = tf.ones([3,3])			# 全1型，二阶
a3 = tf.fill([3,3,3], 3)		# 指定数值，三阶
print(a1, a2, a3)
```
在创建张量时，维度格式按照：一维直接写个数，二维用`[行数，列数]` ，多维就用`[n,m,,j,k,l......]`
有时候我们还会需要一些随机数来作为初始张量，这个随机数一般希望他是符合正态分布（高斯分布）或者均匀分布的：
```
a1 = tf.random.normal([2,2], mean=0, stddev=1)  # 生成正态分布随机数张量，输入参数：维度，均值，标准差
a2 = tf.random.truncated_normal([2,2], mean=0, stddev=1) # 生成截断式正态分布随机数张量，输入参数：维度，均值，标准差，输出数据均在mean±2*stddev之内
print(a1, a2)
```
正态分布服从：
$$ 
\Phi (\mu ,\delta )=\frac{1}{\sqrt{2\pi } \delta} \times e^{-\frac{{(x-\mu)^2} }{2 \delta} }  
$$
## 梯度下降计算程序演示

程序环境
```
cuda = 11.2
python=3.7
numpy==1.19.5
matplotlib== 3.5.3
notebook==6.4.12
tensorflow==2.6.0
```
在下面的程序中我们手动创建一个值为5的张量，并且设置损失函数为
$$
\ f(x)=(x+1)^{2} 
$$ 
```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots()
# 定义变量
lr = [0.6, 0.2, 0.01] # 设置了三个学习率
epoch = 50 # 迭代50次
losses = []
for lrs in lr:
    wegt = tf.Variable(tf.constant(5, dtype=tf.float32))	# 定义了值为5，数据类型为float32
    for epoch in range(epoch):
        with tf.GradientTape() as tape:	# 创建上下文管理器，用于记录网络训练的过程
            loss = tf.square(wegt + 1)	# 计算损失函数的值，在tf.GradientTape上下文中，损失函数是loss = (wegt + 1)的平方
        grads = tape.gradient(loss, wegt)	 # 利用上下文管理器记录的信息，gradient()函数自动求微分，计算损失相对于权重 wegt 的梯度

        wegt.assign_sub(lrs * grads)	# 跟新权重：assign_sub()函数等效是自减=> wegt=wegt-lr * grads
        print("After %s epoch, w is %f, loss is %f" % (epoch, wegt.numpy(), loss))
        losses.append(loss.numpy())
    ax.plot(np.arange(0, epoch + 1, 1), losses, label="lr={}".format(lrs))
    losses.clear()

ax.set(xlabel='Epoch', ylabel='Loss', title='Gradient Descent')
ax.legend(loc='upper right')
ax.grid()
plt.show()
```
可以看到计算得到的wegt权重值不断缩小，loss不断接近于0，且学习率越小w收敛的速度越慢。当然还可以尝试将学习率改为0.99，会看到学习率过大的时候w参数同样不容易稳定下来。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/012b20a8e70046febd4c51ba5f276f41.png)


